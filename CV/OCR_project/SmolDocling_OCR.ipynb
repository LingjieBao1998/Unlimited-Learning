{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingjiebao/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from docling_core.types.doc import DoclingDocument\n",
    "from docling_core.types.doc.document import DocTagsDocument\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"ds4sd/SmolDocling-256M-preview\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #_attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"Snipaste_2025-04-20_20-07-12.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "inputs = inputs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`resolution_max_side` cannot be larger than `max_image_size`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B5090_local/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Prepare inputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B5090_local/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m prompt \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B5090_local/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor(text\u001b[39m=\u001b[39;49mprompt, images\u001b[39m=\u001b[39;49m[image], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B5090_local/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B5090_local/home/lingjiebao/project/Unlimited-Learning/CV/OCR_note/SmolDocling_OCR.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Generate outputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/processing_idefics3.py:262\u001b[0m, in \u001b[0;36mIdefics3Processor.__call__\u001b[0;34m(self, images, text, audio, videos, image_seq_len, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Load images if they are URLs\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     images \u001b[39m=\u001b[39m [[load_image(im) \u001b[39mif\u001b[39;00m is_url(im) \u001b[39melse\u001b[39;00m im \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m sample] \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m images]\n\u001b[0;32m--> 262\u001b[0m     image_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moutput_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mimages_kwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    263\u001b[0m     inputs\u001b[39m.\u001b[39mupdate(image_inputs)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/image_processing_idefics3.py:777\u001b[0m, in \u001b[0;36mIdefics3ImageProcessor.preprocess\u001b[0;34m(self, images, do_convert_rgb, do_resize, size, resample, do_image_splitting, do_rescale, max_image_size, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, return_row_col_info, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid channel dimension format \u001b[39m\u001b[39m{\u001b[39;00minput_data_format\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    776\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 777\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         [\n\u001b[1;32m    779\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39msize, resample\u001b[39m=\u001b[39mresample, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    780\u001b[0m             \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    781\u001b[0m         ]\n\u001b[1;32m    782\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    783\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m do_image_splitting:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# We first resize both height and width of each image to the nearest max_image_size multiple, disregarding the aspect ratio\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39m# for size=(10, max_image_size) -> rescaled_size=(max_image_size, max_image_size)\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     \u001b[39m# for size=(11, max_image_size+1) -> rescaled_size=(max_image_size, max_image_size*2)\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         [\n\u001b[1;32m    791\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_for_vision_encoder(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    797\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/image_processing_idefics3.py:778\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid channel dimension format \u001b[39m\u001b[39m{\u001b[39;00minput_data_format\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    776\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[1;32m    777\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 778\u001b[0m         [\n\u001b[1;32m    779\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39msize, resample\u001b[39m=\u001b[39mresample, input_data_format\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    780\u001b[0m             \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    781\u001b[0m         ]\n\u001b[1;32m    782\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    783\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m do_image_splitting:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# We first resize both height and width of each image to the nearest max_image_size multiple, disregarding the aspect ratio\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39m# for size=(10, max_image_size) -> rescaled_size=(max_image_size, max_image_size)\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     \u001b[39m# for size=(11, max_image_size+1) -> rescaled_size=(max_image_size, max_image_size*2)\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         [\n\u001b[1;32m    791\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_for_vision_encoder(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    797\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/image_processing_idefics3.py:779\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid channel dimension format \u001b[39m\u001b[39m{\u001b[39;00minput_data_format\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    776\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[1;32m    777\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    778\u001b[0m         [\n\u001b[0;32m--> 779\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image\u001b[39m=\u001b[39;49mimage, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample, input_data_format\u001b[39m=\u001b[39;49minput_data_format)\n\u001b[1;32m    780\u001b[0m             \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    781\u001b[0m         ]\n\u001b[1;32m    782\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    783\u001b[0m     ]\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m do_image_splitting:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# We first resize both height and width of each image to the nearest max_image_size multiple, disregarding the aspect ratio\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39m# for size=(10, max_image_size) -> rescaled_size=(max_image_size, max_image_size)\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     \u001b[39m# for size=(11, max_image_size+1) -> rescaled_size=(max_image_size, max_image_size*2)\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     images_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         [\n\u001b[1;32m    791\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_for_vision_encoder(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m images_list\n\u001b[1;32m    797\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/image_processing_idefics3.py:393\u001b[0m, in \u001b[0;36mIdefics3ImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m data_format \u001b[39m=\u001b[39m input_data_format \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data_format\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlongest_edge\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size:\n\u001b[0;32m--> 393\u001b[0m     size \u001b[39m=\u001b[39m get_resize_output_image_size(\n\u001b[1;32m    394\u001b[0m         image, resolution_max_side\u001b[39m=\u001b[39;49msize[\u001b[39m\"\u001b[39;49m\u001b[39mlongest_edge\u001b[39;49m\u001b[39m\"\u001b[39;49m], input_data_format\u001b[39m=\u001b[39;49minput_data_format\n\u001b[1;32m    395\u001b[0m     )\n\u001b[1;32m    396\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m size:\n\u001b[1;32m    397\u001b[0m     size \u001b[39m=\u001b[39m (size[\u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m], size[\u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/idefics3/image_processing_idefics3.py:139\u001b[0m, in \u001b[0;36mget_resize_output_image_size\u001b[0;34m(image, resolution_max_side, max_image_size, input_data_format)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mGet the output size of the image after resizing given a dictionary specifying the max and min sizes.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m    The output size of the image after resizing.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m resolution_max_side \u001b[39m>\u001b[39m max_image_size:\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`resolution_max_side` cannot be larger than `max_image_size`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m height, width \u001b[39m=\u001b[39m get_image_size(image, channel_dim\u001b[39m=\u001b[39minput_data_format)\n\u001b[1;32m    143\u001b[0m \u001b[39m# Find the output size, when rescaling the longest edge to max_len and preserving the aspect ratio\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `resolution_max_side` cannot be larger than `max_image_size`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from docling_core.types.doc import DoclingDocument\n",
    "\n",
    "from docling_core.types.doc.document import DocTagsDocument\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load images\n",
    "\n",
    "# Initialize processor and model\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "\n",
    "    \"ds4sd/SmolDocling-256M-preview\",\n",
    "\n",
    "    torch_dtype=torch.bfloat16,\n",
    "\n",
    "    # _attn_implementation=\"flash_attention_2\"# if DEVICE == \"cuda\" else \"eager\",\n",
    "\n",
    ").to(DEVICE)\n",
    "\n",
    "model.device\n",
    "\n",
    "# Load images\n",
    "\n",
    "image = load_image(\"https://user-images.githubusercontent.com/12294956/47312583-697cfe00-d65a-11e8-930a-e15fd67a5bb1.png\")\n",
    "\n",
    "# Create input messages\n",
    "\n",
    "messages = [\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "\n",
    "        \"content\": [\n",
    "\n",
    "            {\"type\": \"image\"},\n",
    "\n",
    "            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
    "\n",
    "        ]\n",
    "\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "# Generate outputs\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
    "\n",
    "prompt_length = inputs.input_ids.shape[1]\n",
    "\n",
    "trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
    "\n",
    "doctags = processor.batch_decode(\n",
    "\n",
    "    trimmed_generated_ids,\n",
    "\n",
    "    skip_special_tokens=False,\n",
    "\n",
    ")[0].lstrip()\n",
    "\n",
    "# Populate document\n",
    "\n",
    "doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
    "\n",
    "print(doctags)\n",
    "\n",
    "# create a docling document\n",
    "\n",
    "doc = DoclingDocument(name=\"Document\")\n",
    "\n",
    "doc.load_from_doctags(doctags_doc)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(doc.export_to_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
